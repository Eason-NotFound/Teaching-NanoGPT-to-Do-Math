{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3bb2b0c7",
      "metadata": {},
      "source": [
        "## Contribution:\n",
        "All works are discussed and wrote by three of us together"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "124a869a",
      "metadata": {
        "id": "124a869a"
      },
      "source": [
        "### Step 1: Install necesscary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b82f8f1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-20T07:14:44.796390Z",
          "start_time": "2025-10-20T07:14:43.343257Z"
        },
        "id": "3b82f8f1",
        "outputId": "69b59ed4-9e06-47e7-dc78-92425c3e9288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (3.9.4)\r\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\r\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\r\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (4.59.2)\r\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\r\n",
            "Requirement already satisfied: numpy>=1.23 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (2.0.2)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (25.0)\r\n",
            "Requirement already satisfied: pillow>=8 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (11.3.0)\r\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (3.2.4)\r\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\r\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\r\n",
            "Requirement already satisfied: zipp>=3.1.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\r\n",
            "Requirement already satisfied: six>=1.5 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
            "Requirement already satisfied: torch in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (2.8.0)\r\n",
            "Requirement already satisfied: numpy in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (2.0.2)\r\n",
            "Requirement already satisfied: transformers in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (4.56.1)\r\n",
            "Requirement already satisfied: datasets in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (4.1.0)\r\n",
            "Requirement already satisfied: tiktoken in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (0.11.0)\r\n",
            "Requirement already satisfied: wandb in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (0.21.4)\r\n",
            "Requirement already satisfied: tqdm in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (4.67.1)\r\n",
            "Requirement already satisfied: filelock in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from torch) (3.19.1)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from torch) (4.15.0)\r\n",
            "Requirement already satisfied: sympy>=1.13.3 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from torch) (1.14.0)\r\n",
            "Requirement already satisfied: networkx in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
            "Requirement already satisfied: jinja2 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from torch) (3.1.6)\r\n",
            "Requirement already satisfied: fsspec in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from torch) (2025.9.0)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from transformers) (0.35.0)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from transformers) (25.0)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from transformers) (6.0.2)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from transformers) (2025.9.1)\r\n",
            "Requirement already satisfied: requests in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from transformers) (2.32.5)\r\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from transformers) (0.22.0)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from transformers) (0.6.2)\r\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\r\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from datasets) (21.0.0)\r\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from datasets) (0.4.0)\r\n",
            "Requirement already satisfied: pandas in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from datasets) (2.3.2)\r\n",
            "Requirement already satisfied: xxhash in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from datasets) (3.5.0)\r\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from datasets) (0.70.16)\r\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\r\n",
            "Requirement already satisfied: click>=8.0.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from wandb) (8.1.8)\r\n",
            "Requirement already satisfied: eval-type-backport in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from wandb) (0.2.2)\r\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from wandb) (3.1.45)\r\n",
            "Requirement already satisfied: platformdirs in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from wandb) (4.4.0)\r\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from wandb) (6.32.1)\r\n",
            "Requirement already satisfied: pydantic<3 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from wandb) (2.11.9)\r\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from wandb) (2.38.0)\r\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from pydantic<3->wandb) (0.7.0)\r\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from pydantic<3->wandb) (2.33.2)\r\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from pydantic<3->wandb) (0.4.1)\r\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from requests->transformers) (3.4.3)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from requests->transformers) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from requests->transformers) (2.5.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from requests->transformers) (2025.8.3)\r\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\r\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\r\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\r\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\r\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\r\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\r\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\r\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\r\n",
            "Requirement already satisfied: six>=1.5 in /Users/User/Desktop/Dream_Started/SC3000/LAB/Lab1/SC3000_Project/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2d9de0",
      "metadata": {
        "id": "6c2d9de0"
      },
      "source": [
        "### Step 2: Package imports and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "876dd92d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-20T07:14:44.804614Z",
          "start_time": "2025-10-20T07:14:44.799780Z"
        },
        "id": "876dd92d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pickle\n",
        "from model import GPT, GPTConfig\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "# Configuration\n",
        "beta = 0.5\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "base_lr = 1e-4\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "max_length =64\n",
        "num_samples = 1\n",
        "max_new_tokens = 200\n",
        "temperature = 0.8\n",
        "top_k = 200\n",
        "# tokenizer\n",
        "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7d35e6",
      "metadata": {
        "id": "4c7d35e6"
      },
      "source": [
        "### Step 3: Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d03655c3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-20T07:14:44.810543Z",
          "start_time": "2025-10-20T07:14:44.806900Z"
        },
        "id": "d03655c3"
      },
      "outputs": [],
      "source": [
        "def compute_logprob(input_ids):\n",
        "    inputs = input_ids[:, :-1]\n",
        "    targets = input_ids[:, 1:]\n",
        "    logits, _ = gpt(inputs, full_seq=True)\n",
        "    B, T, V = logits.size()\n",
        "    logits_flat = logits.reshape(-1, V)\n",
        "    targets_flat = targets.reshape(-1)\n",
        "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
        "    loss = loss.reshape(B, T)\n",
        "    attention_mask = (targets != 0).float()\n",
        "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
        "    return -loss\n",
        "\n",
        "def pad_or_truncate(seq, max_length):\n",
        "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
        "\n",
        "def get_batches(lines, batch_size):\n",
        "    random.shuffle(lines)\n",
        "    #for l in lines:\n",
        "    #    print(l[1])\n",
        "    for i in range(0, len(lines), batch_size):\n",
        "        batch = lines[i:i+batch_size]\n",
        "        if len(batch) < batch_size:\n",
        "            continue\n",
        "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
        "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
        "        yield neg_tensor, pos_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9d9eba",
      "metadata": {
        "id": "fc9d9eba"
      },
      "source": [
        "### Step 4: Load the pretrained NanoGPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceae772a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-20T07:14:44.988473Z",
          "start_time": "2025-10-20T07:14:44.813229Z"
        },
        "id": "ceae772a",
        "outputId": "392c448b-e167-49ff-de55-1803f6b5100b",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(74, 348)\n",
              "    (wpe): Embedding(256, 348)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
              "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
        "gptconf = GPTConfig(**ckpt['model_args'])\n",
        "gpt = GPT(gptconf)\n",
        "state_dict = ckpt['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k in list(state_dict.keys()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "gpt.to(device).train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0feafc5a",
      "metadata": {
        "id": "0feafc5a"
      },
      "source": [
        "### Step 5: Load Data (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7edf3d44",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-20T07:14:45.037511Z",
          "start_time": "2025-10-20T07:14:44.992143Z"
        },
        "id": "7edf3d44"
      },
      "outputs": [],
      "source": [
        "# Load data from ./pos_neg_pairs.json\n",
        "#with open(\"./pos_neg_pairs.json\", \"r\") as f:\n",
        "#    lines = json.load(f)\n",
        "\n",
        "with open(\"pos_neg_pairs.json\", \"r\") as f:\n",
        "    lines = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e5f81f",
      "metadata": {
        "id": "c2e5f81f"
      },
      "source": [
        "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0c400f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-20T07:14:45.045295Z",
          "start_time": "2025-10-20T07:14:45.041718Z"
        },
        "id": "df0c400f",
        "outputId": "7c7462c4-019c-45e1-a7bf-30a8255f33b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer ready. learning_rate=3.0e-05\n"
          ]
        }
      ],
      "source": [
        "# recommend to use the AdamW optimizer\n",
        "# We freeze the word and position embeddings to keep the model’s core vocabulary stable.\n",
        "# This helps prevent the model from “forgetting” basic language patterns while we fine-tune other layers.\n",
        "\n",
        "from torch import optim\n",
        "# To get better text output, we embed words\n",
        "for name, param in gpt.named_parameters():\n",
        "# 'wte' = word token embeddings, 'wpe' = positional embeddings\n",
        "    if 'wte' in name or 'wpe' in name:\n",
        "        param.requires_grad = False\n",
        "# Define the optimizer — AdamW usually works well for fine-tuning\n",
        "# transformer models.\n",
        "model = 3e-5\n",
        "optimizer = optim.AdamW(\n",
        "    (p for p in gpt.parameters() if p.requires_grad), # only update unfrozen params\n",
        "    lr=model,\n",
        "# decoupled weight decay for better generalization\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Optimizer ready. learning_rate={model:.1e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "keObN0RRTluE",
      "metadata": {
        "id": "keObN0RRTluE"
      },
      "source": [
        "For this part, we studied the lab example to use DPO as our training method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b66199",
      "metadata": {
        "id": "52b66199"
      },
      "source": [
        "### Step 7: Begin training (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d4ebeb4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-20T09:04:58.934158Z",
          "start_time": "2025-10-20T07:14:45.051454Z"
        },
        "id": "1d4ebeb4",
        "outputId": "6f92e3fb-b5c5-49f3-f179-c04fba20b281",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[E1] step 1562/1562 | Loss=1.068 DPO=0.454 LM=0.701 Δ=13.87: : 1562it [22:08,  1.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpoTryNew100.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[E2] step 1562/1562 | Loss=0.955 DPO=0.438 LM=0.525 Δ=14.99: : 1562it [22:05,  1.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpoTryNew100.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[E3] step 1562/1562 | Loss=0.899 DPO=0.434 LM=0.491 Δ=15.26: : 1562it [21:51,  1.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpoTryNew100.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[E4] step 1562/1562 | Loss=0.845 DPO=0.426 LM=0.408 Δ=15.88: : 1562it [22:05,  1.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpoTryNew100.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[E5] step 1562/1562 | Loss=0.803 DPO=0.414 LM=0.439 Δ=16.76: : 1562it [22:03,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to ./dpoTryNew100.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "total_steps = len(lines) // batch_size\n",
        "for epoch in range(epochs):\n",
        "    pbar = tqdm(get_batches(lines, batch_size)) # prepare shuffled batches for training\n",
        "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
        "     # These parameters control how strongly we weight different loss terms.\n",
        "        ###########################################################\n",
        "        # Please complete the training code here!\n",
        "        # Examples:\n",
        "        # ...\n",
        "        # neg_logprob\n",
        "        # pos_logprob\n",
        "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
        "        # ...\n",
        "        ###########################################################\n",
        "        beta = 12.5         # larger beta can flat the sigmoid\n",
        "        lm_coef = 0.075          # weight on language-model term\n",
        "        gp = 0.9      # gradient clipping to avoid explosion\n",
        "\n",
        "\n",
        "        use_amp = torch.cuda.is_available() # enable automatic mixed precision if using GPU\n",
        "\n",
        "\n",
        "        with torch.amp.autocast('cuda', enabled=use_amp): # Compute average token log-probabilities for positive and negative samples\n",
        "            pos = compute_logprob(pos_tensor)\n",
        "            neg = compute_logprob(neg_tensor)\n",
        "\n",
        "\n",
        "            # DPO loss encourages the model to prefer positive samples over negative ones\n",
        "            delta = 0.5 * (pos - neg) # balance the training to avoid overflow (since we are using a high beta value)\n",
        "            dpo= -F.logsigmoid(delta / beta).mean()\n",
        "\n",
        "\n",
        "            # LM loss keeps the generated text fluent by maximizing log-prob of the positive examples\n",
        "            lm = (-pos).mean() # increase the fluency of text output\n",
        "            # Small L2 regularization helps keep weights from drifting too far\n",
        "            l2 = 1e-4 * sum(p.norm() ** 2 for p in gpt.parameters())\n",
        "\n",
        "\n",
        "            loss = dpo + lm_coef * lm + l2 # Combine everything into a single loss value\n",
        "\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "\n",
        "        # since the difference between positive and negative training samples is large\n",
        "        loss.backward()  # backpropagate gradients through the model\n",
        "        # Clip gradients to avoid instability from large updates\n",
        "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), gp)\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        # see the training process\n",
        "        with torch.no_grad():\n",
        "            delta = (pos - neg)\n",
        "            pbar.set_description(\n",
        "                f\"[E{epoch+1}/E{epochs}] step {step+1}/{total_steps} | \"\n",
        "                f\"Loss={loss.item():.3f} DPO={dpo.item():.3f} LM={lm.item():.3f} \"\n",
        "                f\"Δ={delta.mean().item():.2f}\" # search online for delta symbol\n",
        "            )\n",
        "\n",
        "\n",
        "    # Save a checkpoint after each epoch so training can be resumed later if needed\n",
        "    ckpt_path = f\"dpo.pt\"\n",
        "    torch.save({\n",
        "        \"model_state_dict\": gpt.state_dict(),\n",
        "        \"model_args\": ckpt['model_args'],\n",
        "    }, ckpt_path)\n",
        "    print(f\"Saved checkpoint to {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32aa8984a1348fd9",
      "metadata": {
        "id": "32aa8984a1348fd9"
      },
      "source": [
        "# Originality explanation for training part\n",
        "## We have changed different set of parameters to try to get better results.\n",
        "At the first time, beta is set to 1, but the final result is like following: Prompt: 17+19=? Output: e answer is 1037. After our discussion about this problem, we found that during the training process, the difference between positive and negative log probabilities is too large with a delta of nearly 13, at the same time, dpo value is very small, almost 0 which means the learning almost stops. Thus, we decided to raise beta value to flat the sigmoid function as well as adding a coefficient before the difference \"delta = 0.5 * (pos - neg)\" to cut down the difference value.\n",
        "So that we decided to try different sets of parameters, before which we set epoches to 1 for quick test. After trying many different sets of these three parameters (beta, lm_coef, delta coefficient), we finally set beta to 12.5, lm_coef to 0.075 and delta coefficient to 0.5.\n",
        "## We have also added several measures to avoid overfitting and output chaos.\n",
        "As mentioned before, we encountered the large beta and output text chaos problem. To avoid these two problems, we firstly added L2 regularization and gradiant clipping to prevent overfitting. Secondly, we included the loss of lm (lm) and lm_coef to improve the fluency of the output text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b7f2ab",
      "metadata": {
        "id": "48b7f2ab"
      },
      "source": [
        "### Step 8: Begin testing (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09027262",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-20T09:21:05.213344Z",
          "start_time": "2025-10-20T09:21:03.150895Z"
        },
        "id": "09027262",
        "outputId": "0ce2a75e-dc2c-4821-de54-9a5d48d612f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: 17+19=?\n",
            "Output: The answer is 108 because 17+19 equals 108.\n",
            "\n",
            "Prompt: 3*17=?\n",
            "Output: The answer is 32 because 3*17 equals 32.\n",
            "\n",
            "Prompt: 72/4=?\n",
            "Output: The answer is -464 because -1109244/-44 equals -4\n",
            "\n",
            "Prompt: 72-x=34,x=?\n",
            "Output: *he answer is 13 because 72-34 equals 13.\n",
            "\n",
            "Prompt: x*11=44,x=?\n",
            "Output: The answer is 14 because 44-11 equals 14.\n",
            "\n",
            "Prompt: 3*17=?\n",
            "Output: The answer is 38 because 3*17 equals 38.\n",
            "\n",
            "Prompt: 72/4=?\n",
            "Output: The answer is -3253 because -148202/-44 equals -3\n",
            "\n",
            "Prompt: 72-x=34,x=?\n",
            "Output: The answer is 3 because 72-34 equals 3.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model\n",
        "ckpt_path = \"dpo.pt\"\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "# Safely move model to whichever device is available\n",
        "#add following\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "gpt = GPT(gptconf).to(device)\n",
        "#removed following\n",
        "#gpt = GPT(gptconf).cuda()\n",
        "# Handle possible naming differences between checkpoints\n",
        "try:\n",
        "    state_dict = checkpoint['model']\n",
        "except:\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "\n",
        "# Clean up any old prefixes left over from distributed training\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "\n",
        "# Load weights into our model\n",
        "gpt.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "# switch to eval mode so dropout etc. are disabled\n",
        "gpt.eval()\n",
        "\n",
        "\n",
        "# Run a few test prompts to check that generation works after fine-tuning\n",
        "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
        "with torch.no_grad():\n",
        "    for prompt in test_set:\n",
        "        prompt_ids = encode(prompt)\n",
        "        ###########################################################\n",
        "        # Please complete the test code here!\n",
        "        # ...\n",
        "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "        # ...\n",
        "        x = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
        "\n",
        "\n",
        "        # Generate up to 50 new tokens with some randomness (temperature = 0.8)\n",
        "        y_tuple = gpt.generate(x, max_new_tokens=50, temperature=0.8, top_k=50)\n",
        "        y = y_tuple[0]\n",
        "        y = y[0].tolist()\n",
        "\n",
        "\n",
        "        # Decode the new tokens back into readable text\n",
        "        generated_ids = y[len(prompt_ids):]\n",
        "        output_text = decode(generated_ids).strip()\n",
        "        print(f\"Prompt: {prompt}\\nOutput: {output_text}\\n\")\n",
        "        ###########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r-gHgh0vS-IN",
      "metadata": {
        "id": "r-gHgh0vS-IN"
      },
      "source": [
        "# Generation code for self generate json negative pair\n",
        "\n",
        "\n",
        "Here, we tried to use different negative-positive pair, by generating the negative pair from existing model, and using it to further fine tune the model, in theory improving it each time.\n",
        "\n",
        "\n",
        "Before this we had also tried generation with 1 million negative and 100 thousand negative pairs. Eventually, we are using 100 thousand positive negative pairs. All other datasets are attached in the other mail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "El41XvEu0O2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El41XvEu0O2e",
        "outputId": "15ef9a43-1cf7-419b-8908-948b20cbe0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 100000 self-evaluated samples ...\n",
            "Generated 100/100000\n",
            "Generated 200/100000\n",
            "Generated 300/100000\n",
            "Generated 400/100000\n",
            "Generated 500/100000\n",
            "Generated 600/100000\n",
            "Generated 700/100000\n",
            "Generated 800/100000\n",
            "Generated 900/100000\n",
            "Generated 1000/100000\n",
            "Generated 1100/100000\n",
            "Generated 1200/100000\n",
            "Generated 1300/100000\n",
            "Generated 1400/100000\n",
            "Generated 1500/100000\n",
            "Generated 1600/100000\n",
            "Generated 1700/100000\n",
            "Generated 1800/100000\n",
            "Generated 1900/100000\n",
            "Generated 2000/100000\n",
            "Generated 2100/100000\n",
            "Generated 2200/100000\n",
            "Generated 2300/100000\n",
            "Generated 2400/100000\n",
            "Generated 2500/100000\n",
            "Generated 2600/100000\n",
            "Generated 2700/100000\n",
            "Generated 2800/100000\n",
            "Generated 2900/100000\n",
            "Generated 3000/100000\n",
            "Generated 3100/100000\n",
            "Generated 3200/100000\n",
            "Generated 3300/100000\n",
            "Generated 3400/100000\n",
            "Generated 3500/100000\n",
            "Generated 3600/100000\n",
            "Generated 3700/100000\n",
            "Generated 3800/100000\n",
            "Generated 3900/100000\n",
            "Generated 4000/100000\n",
            "Generated 4100/100000\n",
            "Generated 4200/100000\n",
            "Generated 4300/100000\n",
            "Generated 4400/100000\n",
            "Generated 4500/100000\n",
            "Generated 4600/100000\n",
            "Generated 4700/100000\n",
            "Generated 4800/100000\n",
            "Generated 4900/100000\n",
            "Generated 5000/100000\n",
            "Generated 5100/100000\n",
            "Generated 5200/100000\n",
            "Generated 5300/100000\n",
            "Generated 5400/100000\n",
            "Generated 5500/100000\n",
            "Generated 5600/100000\n",
            "Generated 5700/100000\n",
            "Generated 5800/100000\n",
            "Generated 5900/100000\n",
            "Generated 6000/100000\n",
            "Generated 6100/100000\n",
            "Generated 6200/100000\n",
            "Generated 6300/100000\n",
            "Generated 6400/100000\n",
            "Generated 6500/100000\n",
            "Generated 6600/100000\n",
            "Generated 6700/100000\n",
            "Generated 6800/100000\n",
            "Generated 6900/100000\n",
            "Generated 7000/100000\n",
            "Generated 7100/100000\n",
            "Generated 7200/100000\n",
            "Generated 7300/100000\n",
            "Generated 7400/100000\n",
            "Generated 7500/100000\n",
            "Generated 7600/100000\n",
            "Generated 7700/100000\n",
            "Generated 7800/100000\n",
            "Generated 7900/100000\n",
            "Generated 8000/100000\n",
            "Generated 8100/100000\n",
            "Generated 8200/100000\n",
            "Generated 8300/100000\n",
            "Generated 8400/100000\n",
            "Generated 8500/100000\n",
            "Generated 8600/100000\n",
            "Generated 8700/100000\n",
            "Generated 8800/100000\n",
            "Generated 8900/100000\n",
            "Generated 9000/100000\n",
            "Generated 9100/100000\n",
            "Generated 9200/100000\n",
            "Generated 9300/100000\n",
            "Generated 9400/100000\n",
            "Generated 9500/100000\n",
            "Generated 9600/100000\n",
            "Generated 9700/100000\n",
            "Generated 9800/100000\n",
            "Generated 9900/100000\n",
            "Generated 10000/100000\n",
            "Generated 10100/100000\n",
            "Generated 10200/100000\n",
            "Generated 10300/100000\n",
            "Generated 10400/100000\n",
            "Generated 10500/100000\n",
            "Generated 10600/100000\n",
            "Generated 10700/100000\n",
            "Generated 10800/100000\n",
            "Generated 10900/100000\n",
            "Generated 11000/100000\n",
            "Generated 11100/100000\n",
            "Generated 11200/100000\n",
            "Generated 11300/100000\n",
            "Generated 11400/100000\n",
            "Generated 11500/100000\n",
            "Generated 11600/100000\n",
            "Generated 11700/100000\n",
            "Generated 11800/100000\n",
            "Generated 11900/100000\n",
            "Generated 12000/100000\n",
            "Generated 12100/100000\n",
            "Generated 12200/100000\n",
            "Generated 12300/100000\n",
            "Generated 12400/100000\n",
            "Generated 12500/100000\n",
            "Generated 12600/100000\n",
            "Generated 12700/100000\n",
            "Generated 12800/100000\n",
            "Generated 12900/100000\n",
            "Generated 13000/100000\n",
            "Generated 13100/100000\n",
            "Generated 13200/100000\n",
            "Generated 13300/100000\n",
            "Generated 13400/100000\n",
            "Generated 13500/100000\n",
            "Generated 13600/100000\n",
            "Generated 13700/100000\n",
            "Generated 13800/100000\n",
            "Generated 13900/100000\n",
            "Generated 14000/100000\n",
            "Generated 14100/100000\n",
            "Generated 14200/100000\n",
            "Generated 14300/100000\n",
            "Generated 14400/100000\n",
            "Generated 14500/100000\n",
            "Generated 14600/100000\n",
            "Generated 14700/100000\n",
            "Generated 14800/100000\n",
            "Generated 14900/100000\n",
            "Generated 15000/100000\n",
            "Generated 15100/100000\n",
            "Generated 15200/100000\n",
            "Generated 15300/100000\n",
            "Generated 15400/100000\n",
            "Generated 15500/100000\n",
            "Generated 15600/100000\n",
            "Generated 15700/100000\n",
            "Generated 15800/100000\n",
            "Generated 15900/100000\n",
            "Generated 16000/100000\n",
            "Generated 16100/100000\n",
            "Generated 16200/100000\n",
            "Generated 16300/100000\n",
            "Generated 16400/100000\n",
            "Generated 16500/100000\n",
            "Generated 16600/100000\n",
            "Generated 16700/100000\n",
            "Generated 16800/100000\n",
            "Generated 16900/100000\n",
            "Generated 17000/100000\n",
            "Generated 17100/100000\n",
            "Generated 17200/100000\n",
            "Generated 17300/100000\n",
            "Generated 17400/100000\n",
            "Generated 17500/100000\n",
            "Generated 17600/100000\n",
            "Generated 17700/100000\n",
            "Generated 17800/100000\n",
            "Generated 17900/100000\n",
            "Generated 18000/100000\n",
            "Generated 18100/100000\n",
            "Generated 18200/100000\n",
            "Generated 18300/100000\n",
            "Generated 18400/100000\n",
            "Generated 18500/100000\n",
            "Generated 18600/100000\n",
            "Generated 18700/100000\n",
            "Generated 18800/100000\n",
            "Generated 18900/100000\n",
            "Generated 19000/100000\n",
            "Generated 19100/100000\n",
            "Generated 19200/100000\n",
            "Generated 19300/100000\n",
            "Generated 19400/100000\n",
            "Generated 19500/100000\n",
            "Generated 19600/100000\n",
            "Generated 19700/100000\n",
            "Generated 19800/100000\n",
            "Generated 19900/100000\n",
            "Generated 20000/100000\n",
            "Generated 20100/100000\n",
            "Generated 20200/100000\n",
            "Generated 20300/100000\n",
            "Generated 20400/100000\n",
            "Generated 20500/100000\n",
            "Generated 20600/100000\n",
            "Generated 20700/100000\n",
            "Generated 20800/100000\n",
            "Generated 20900/100000\n",
            "Generated 21000/100000\n",
            "Generated 21100/100000\n",
            "Generated 21200/100000\n",
            "Generated 21300/100000\n",
            "Generated 21400/100000\n",
            "Generated 21500/100000\n",
            "Generated 21600/100000\n",
            "Generated 21700/100000\n",
            "Generated 21800/100000\n",
            "Generated 21900/100000\n",
            "Generated 22000/100000\n",
            "Generated 22100/100000\n",
            "Generated 22200/100000\n",
            "Generated 22300/100000\n",
            "Generated 22400/100000\n",
            "Generated 22500/100000\n",
            "Generated 22600/100000\n",
            "Generated 22700/100000\n",
            "Generated 22800/100000\n",
            "Generated 22900/100000\n",
            "Generated 23000/100000\n",
            "Generated 23100/100000\n",
            "Generated 23200/100000\n",
            "Generated 23300/100000\n",
            "Generated 23400/100000\n",
            "Generated 23500/100000\n",
            "Generated 23600/100000\n",
            "Generated 23700/100000\n",
            "Generated 23800/100000\n",
            "Generated 23900/100000\n",
            "Generated 24000/100000\n",
            "Generated 24100/100000\n",
            "Generated 24200/100000\n",
            "Generated 24300/100000\n",
            "Generated 24400/100000\n",
            "Generated 24500/100000\n",
            "Generated 24600/100000\n",
            "Generated 24700/100000\n",
            "Generated 24800/100000\n",
            "Generated 24900/100000\n",
            "Generated 25000/100000\n",
            "Generated 25100/100000\n",
            "Generated 25200/100000\n",
            "Generated 25300/100000\n",
            "Generated 25400/100000\n",
            "Generated 25500/100000\n",
            "Generated 25600/100000\n",
            "Generated 25700/100000\n",
            "Generated 25800/100000\n",
            "Generated 25900/100000\n",
            "Generated 26000/100000\n",
            "Generated 26100/100000\n",
            "Generated 26200/100000\n",
            "Generated 26300/100000\n",
            "Generated 26400/100000\n",
            "Generated 26500/100000\n",
            "Generated 26600/100000\n",
            "Generated 26700/100000\n",
            "Generated 26800/100000\n",
            "Generated 26900/100000\n",
            "Generated 27000/100000\n",
            "Generated 27100/100000\n",
            "Generated 27200/100000\n",
            "Generated 27300/100000\n",
            "Generated 27400/100000\n",
            "Generated 27500/100000\n",
            "Generated 27600/100000\n",
            "Generated 27700/100000\n",
            "Generated 27800/100000\n",
            "Generated 27900/100000\n",
            "Generated 28000/100000\n",
            "Generated 28100/100000\n",
            "Generated 28200/100000\n",
            "Generated 28300/100000\n",
            "Generated 28400/100000\n",
            "Generated 28500/100000\n",
            "Generated 28600/100000\n",
            "Generated 28700/100000\n",
            "Generated 28800/100000\n",
            "Generated 28900/100000\n",
            "Generated 29000/100000\n",
            "Generated 29100/100000\n",
            "Generated 29200/100000\n",
            "Generated 29300/100000\n",
            "Generated 29400/100000\n",
            "Generated 29500/100000\n",
            "Generated 29600/100000\n",
            "Generated 29700/100000\n",
            "Generated 29800/100000\n",
            "Generated 29900/100000\n",
            "Generated 30000/100000\n",
            "Generated 30100/100000\n",
            "Generated 30200/100000\n",
            "Generated 30300/100000\n",
            "Generated 30400/100000\n",
            "Generated 30500/100000\n",
            "Generated 30600/100000\n",
            "Generated 30700/100000\n",
            "Generated 30800/100000\n",
            "Generated 30900/100000\n",
            "Generated 31000/100000\n",
            "Generated 31100/100000\n",
            "Generated 31200/100000\n",
            "Generated 31300/100000\n",
            "Generated 31400/100000\n",
            "Generated 31500/100000\n",
            "Generated 31600/100000\n",
            "Generated 31700/100000\n",
            "Generated 31800/100000\n",
            "Generated 31900/100000\n",
            "Generated 32000/100000\n",
            "Generated 32100/100000\n",
            "Generated 32200/100000\n",
            "Generated 32300/100000\n",
            "Generated 32400/100000\n",
            "Generated 32500/100000\n",
            "Generated 32600/100000\n",
            "Generated 32700/100000\n",
            "Generated 32800/100000\n",
            "Generated 32900/100000\n",
            "Generated 33000/100000\n",
            "Generated 33100/100000\n",
            "Generated 33200/100000\n",
            "Generated 33300/100000\n",
            "Generated 33400/100000\n",
            "Generated 33500/100000\n",
            "Generated 33600/100000\n",
            "Generated 33700/100000\n",
            "Generated 33800/100000\n",
            "Generated 33900/100000\n",
            "Generated 34000/100000\n",
            "Generated 34100/100000\n",
            "Generated 34200/100000\n",
            "Generated 34300/100000\n",
            "Generated 34400/100000\n",
            "Generated 34500/100000\n",
            "Generated 34600/100000\n",
            "Generated 34700/100000\n",
            "Generated 34800/100000\n",
            "Generated 34900/100000\n",
            "Generated 35000/100000\n",
            "Generated 35100/100000\n",
            "Generated 35200/100000\n",
            "Generated 35300/100000\n",
            "Generated 35400/100000\n",
            "Generated 35500/100000\n",
            "Generated 35600/100000\n",
            "Generated 35700/100000\n",
            "Generated 35800/100000\n",
            "Generated 35900/100000\n",
            "Generated 36000/100000\n",
            "Generated 36100/100000\n",
            "Generated 36200/100000\n",
            "Generated 36300/100000\n",
            "Generated 36400/100000\n",
            "Generated 36500/100000\n",
            "Generated 36600/100000\n",
            "Generated 36700/100000\n",
            "Generated 36800/100000\n",
            "Generated 36900/100000\n",
            "Generated 37000/100000\n",
            "Generated 37100/100000\n",
            "Generated 37200/100000\n",
            "Generated 37300/100000\n",
            "Generated 37400/100000\n",
            "Generated 37500/100000\n",
            "Generated 37600/100000\n",
            "Generated 37700/100000\n",
            "Generated 37800/100000\n",
            "Generated 37900/100000\n",
            "Generated 38000/100000\n",
            "Generated 38100/100000\n",
            "Generated 38200/100000\n",
            "Generated 38300/100000\n",
            "Generated 38400/100000\n",
            "Generated 38500/100000\n",
            "Generated 38600/100000\n",
            "Generated 38700/100000\n",
            "Generated 38800/100000\n",
            "Generated 38900/100000\n",
            "Generated 39000/100000\n",
            "Generated 39100/100000\n",
            "Generated 39200/100000\n",
            "Generated 39300/100000\n",
            "Generated 39400/100000\n",
            "Generated 39500/100000\n",
            "Generated 39600/100000\n",
            "Generated 39700/100000\n",
            "Generated 39800/100000\n",
            "Generated 39900/100000\n",
            "Generated 40000/100000\n",
            "Generated 40100/100000\n",
            "Generated 40200/100000\n",
            "Generated 40300/100000\n",
            "Generated 40400/100000\n",
            "Generated 40500/100000\n",
            "Generated 40600/100000\n",
            "Generated 40700/100000\n",
            "Generated 40800/100000\n",
            "Generated 40900/100000\n",
            "Generated 41000/100000\n",
            "Generated 41100/100000\n",
            "Generated 41200/100000\n",
            "Generated 41300/100000\n",
            "Generated 41400/100000\n",
            "Generated 41500/100000\n",
            "Generated 41600/100000\n",
            "Generated 41700/100000\n",
            "Generated 41800/100000\n",
            "Generated 41900/100000\n",
            "Generated 42000/100000\n",
            "Generated 42100/100000\n",
            "Generated 42200/100000\n",
            "Generated 42300/100000\n",
            "Generated 42400/100000\n",
            "Generated 42500/100000\n",
            "Generated 42600/100000\n",
            "Generated 42700/100000\n",
            "Generated 42800/100000\n",
            "Generated 42900/100000\n",
            "Generated 43000/100000\n",
            "Generated 43100/100000\n",
            "Generated 43200/100000\n",
            "Generated 43300/100000\n",
            "Generated 43400/100000\n",
            "Generated 43500/100000\n",
            "Generated 43600/100000\n",
            "Generated 43700/100000\n",
            "Generated 43800/100000\n",
            "Generated 43900/100000\n",
            "Generated 44000/100000\n",
            "Generated 44100/100000\n",
            "Generated 44200/100000\n",
            "Generated 44300/100000\n",
            "Generated 44400/100000\n",
            "Generated 44500/100000\n",
            "Generated 44600/100000\n",
            "Generated 44700/100000\n",
            "Generated 44800/100000\n",
            "Generated 44900/100000\n",
            "Generated 45000/100000\n",
            "Generated 45100/100000\n",
            "Generated 45200/100000\n",
            "Generated 45300/100000\n",
            "Generated 45400/100000\n",
            "Generated 45500/100000\n",
            "Generated 45600/100000\n",
            "Generated 45700/100000\n",
            "Generated 45800/100000\n",
            "Generated 45900/100000\n",
            "Generated 46000/100000\n",
            "Generated 46100/100000\n",
            "Generated 46200/100000\n",
            "Generated 46300/100000\n",
            "Generated 46400/100000\n",
            "Generated 46500/100000\n",
            "Generated 46600/100000\n",
            "Generated 46700/100000\n",
            "Generated 46800/100000\n",
            "Generated 46900/100000\n",
            "Generated 47000/100000\n",
            "Generated 47100/100000\n",
            "Generated 47200/100000\n",
            "Generated 47300/100000\n",
            "Generated 47400/100000\n",
            "Generated 47500/100000\n",
            "Generated 47600/100000\n",
            "Generated 47700/100000\n",
            "Generated 47800/100000\n",
            "Generated 47900/100000\n",
            "Generated 48000/100000\n",
            "Generated 48100/100000\n",
            "Generated 48200/100000\n",
            "Generated 48300/100000\n",
            "Generated 48400/100000\n",
            "Generated 48500/100000\n",
            "Generated 48600/100000\n",
            "Generated 48700/100000\n",
            "Generated 48800/100000\n",
            "Generated 48900/100000\n",
            "Generated 49000/100000\n",
            "Generated 49100/100000\n",
            "Generated 49200/100000\n",
            "Generated 49300/100000\n",
            "Generated 49400/100000\n",
            "Generated 49500/100000\n",
            "Generated 49600/100000\n",
            "Generated 49700/100000\n",
            "Generated 49800/100000\n",
            "Generated 49900/100000\n",
            "Generated 50000/100000\n",
            "Generated 50100/100000\n",
            "Generated 50200/100000\n",
            "Generated 50300/100000\n",
            "Generated 50400/100000\n",
            "Generated 50500/100000\n",
            "Generated 50600/100000\n",
            "Generated 50700/100000\n",
            "Generated 50800/100000\n",
            "Generated 50900/100000\n",
            "Generated 51000/100000\n",
            "Generated 51100/100000\n",
            "Generated 51200/100000\n",
            "Generated 51300/100000\n",
            "Generated 51400/100000\n",
            "Generated 51500/100000\n",
            "Generated 51600/100000\n",
            "Generated 51700/100000\n",
            "Generated 51800/100000\n",
            "Generated 51900/100000\n",
            "Generated 52000/100000\n",
            "Generated 52100/100000\n",
            "Generated 52200/100000\n",
            "Generated 52300/100000\n",
            "Generated 52400/100000\n",
            "Generated 52500/100000\n",
            "Generated 52600/100000\n",
            "Generated 52700/100000\n",
            "Generated 52800/100000\n",
            "Generated 52900/100000\n",
            "Generated 53000/100000\n",
            "Generated 53100/100000\n",
            "Generated 53200/100000\n",
            "Generated 53300/100000\n",
            "Generated 53400/100000\n",
            "Generated 53500/100000\n",
            "Generated 53600/100000\n",
            "Generated 53700/100000\n",
            "Generated 53800/100000\n",
            "Generated 53900/100000\n",
            "Generated 54000/100000\n",
            "Generated 54100/100000\n",
            "Generated 54200/100000\n",
            "Generated 54300/100000\n",
            "Generated 54400/100000\n",
            "Generated 54500/100000\n",
            "Generated 54600/100000\n",
            "Generated 54700/100000\n",
            "Generated 54800/100000\n",
            "Generated 54900/100000\n",
            "Generated 55000/100000\n",
            "Generated 55100/100000\n",
            "Generated 55200/100000\n",
            "Generated 55300/100000\n",
            "Generated 55400/100000\n",
            "Generated 55500/100000\n",
            "Generated 55600/100000\n",
            "Generated 55700/100000\n",
            "Generated 55800/100000\n",
            "Generated 55900/100000\n",
            "Generated 56000/100000\n",
            "Generated 56100/100000\n",
            "Generated 56200/100000\n",
            "Generated 56300/100000\n",
            "Generated 56400/100000\n",
            "Generated 56500/100000\n",
            "Generated 56600/100000\n",
            "Generated 56700/100000\n",
            "Generated 56800/100000\n",
            "Generated 56900/100000\n",
            "Generated 57000/100000\n",
            "Generated 57100/100000\n",
            "Generated 57200/100000\n",
            "Generated 57300/100000\n",
            "Generated 57400/100000\n",
            "Generated 57500/100000\n",
            "Generated 57600/100000\n",
            "Generated 57700/100000\n",
            "Generated 57800/100000\n",
            "Generated 57900/100000\n",
            "Generated 58000/100000\n",
            "Generated 58100/100000\n",
            "Generated 58200/100000\n",
            "Generated 58300/100000\n",
            "Generated 58400/100000\n",
            "Generated 58500/100000\n",
            "Generated 58600/100000\n",
            "Generated 58700/100000\n",
            "Generated 58800/100000\n",
            "Generated 58900/100000\n",
            "Generated 59000/100000\n",
            "Generated 59100/100000\n",
            "Generated 59200/100000\n",
            "Generated 59300/100000\n",
            "Generated 59400/100000\n",
            "Generated 59500/100000\n",
            "Generated 59600/100000\n",
            "Generated 59700/100000\n",
            "Generated 59800/100000\n",
            "Generated 59900/100000\n",
            "Generated 60000/100000\n",
            "Generated 60100/100000\n",
            "Generated 60200/100000\n",
            "Generated 60300/100000\n",
            "Generated 60400/100000\n",
            "Generated 60500/100000\n",
            "Generated 60600/100000\n",
            "Generated 60700/100000\n",
            "Generated 60800/100000\n",
            "Generated 60900/100000\n",
            "Generated 61000/100000\n",
            "Generated 61100/100000\n",
            "Generated 61200/100000\n",
            "Generated 61300/100000\n",
            "Generated 61400/100000\n",
            "Generated 61500/100000\n",
            "Generated 61600/100000\n",
            "Generated 61700/100000\n",
            "Generated 61800/100000\n",
            "Generated 61900/100000\n",
            "Generated 62000/100000\n",
            "Generated 62100/100000\n",
            "Generated 62200/100000\n",
            "Generated 62300/100000\n",
            "Generated 62400/100000\n",
            "Generated 62500/100000\n",
            "Generated 62600/100000\n",
            "Generated 62700/100000\n",
            "Generated 62800/100000\n",
            "Generated 62900/100000\n",
            "Generated 63000/100000\n",
            "Generated 63100/100000\n",
            "Generated 63200/100000\n",
            "Generated 63300/100000\n",
            "Generated 63400/100000\n",
            "Generated 63500/100000\n",
            "Generated 63600/100000\n",
            "Generated 63700/100000\n",
            "Generated 63800/100000\n",
            "Generated 63900/100000\n",
            "Generated 64000/100000\n",
            "Generated 64100/100000\n",
            "Generated 64200/100000\n",
            "Generated 64300/100000\n",
            "Generated 64400/100000\n",
            "Generated 64500/100000\n",
            "Generated 64600/100000\n",
            "Generated 64700/100000\n",
            "Generated 64800/100000\n",
            "Generated 64900/100000\n",
            "Generated 65000/100000\n",
            "Generated 65100/100000\n",
            "Generated 65200/100000\n",
            "Generated 65300/100000\n",
            "Generated 65400/100000\n",
            "Generated 65500/100000\n",
            "Generated 65600/100000\n",
            "Generated 65700/100000\n",
            "Generated 65800/100000\n",
            "Generated 65900/100000\n",
            "Generated 66000/100000\n",
            "Generated 66100/100000\n",
            "Generated 66200/100000\n",
            "Generated 66300/100000\n",
            "Generated 66400/100000\n",
            "Generated 66500/100000\n",
            "Generated 66600/100000\n",
            "Generated 66700/100000\n",
            "Generated 66800/100000\n",
            "Generated 66900/100000\n",
            "Generated 67000/100000\n",
            "Generated 67100/100000\n",
            "Generated 67200/100000\n",
            "Generated 67300/100000\n",
            "Generated 67400/100000\n",
            "Generated 67500/100000\n",
            "Generated 67600/100000\n",
            "Generated 67700/100000\n",
            "Generated 67800/100000\n",
            "Generated 67900/100000\n",
            "Generated 68000/100000\n",
            "Generated 68100/100000\n",
            "Generated 68200/100000\n",
            "Generated 68300/100000\n",
            "Generated 68400/100000\n",
            "Generated 68500/100000\n",
            "Generated 68600/100000\n",
            "Generated 68700/100000\n",
            "Generated 68800/100000\n",
            "Generated 68900/100000\n",
            "Generated 69000/100000\n",
            "Generated 69100/100000\n",
            "Generated 69200/100000\n",
            "Generated 69300/100000\n",
            "Generated 69400/100000\n",
            "Generated 69500/100000\n",
            "Generated 69600/100000\n",
            "Generated 69700/100000\n",
            "Generated 69800/100000\n",
            "Generated 69900/100000\n",
            "Generated 70000/100000\n",
            "Generated 70100/100000\n",
            "Generated 70200/100000\n",
            "Generated 70300/100000\n",
            "Generated 70400/100000\n",
            "Generated 70500/100000\n",
            "Generated 70600/100000\n",
            "Generated 70700/100000\n",
            "Generated 70800/100000\n",
            "Generated 70900/100000\n",
            "Generated 71000/100000\n",
            "Generated 71100/100000\n",
            "Generated 71200/100000\n",
            "Generated 71300/100000\n",
            "Generated 71400/100000\n",
            "Generated 71500/100000\n",
            "Generated 71600/100000\n",
            "Generated 71700/100000\n",
            "Generated 71800/100000\n",
            "Generated 71900/100000\n",
            "Generated 72000/100000\n",
            "Generated 72100/100000\n",
            "Generated 72200/100000\n",
            "Generated 72300/100000\n",
            "Generated 72400/100000\n",
            "Generated 72500/100000\n",
            "Generated 72600/100000\n",
            "Generated 72700/100000\n",
            "Generated 72800/100000\n",
            "Generated 72900/100000\n",
            "Generated 73000/100000\n",
            "Generated 73100/100000\n",
            "Generated 73200/100000\n",
            "Generated 73300/100000\n",
            "Generated 73400/100000\n",
            "Generated 73500/100000\n",
            "Generated 73600/100000\n",
            "Generated 73700/100000\n",
            "Generated 73800/100000\n",
            "Generated 73900/100000\n",
            "Generated 74000/100000\n",
            "Generated 74100/100000\n",
            "Generated 74200/100000\n",
            "Generated 74300/100000\n",
            "Generated 74400/100000\n",
            "Generated 74500/100000\n",
            "Generated 74600/100000\n",
            "Generated 74700/100000\n",
            "Generated 74800/100000\n",
            "Generated 74900/100000\n",
            "Generated 75000/100000\n",
            "Generated 75100/100000\n",
            "Generated 75200/100000\n",
            "Generated 75300/100000\n",
            "Generated 75400/100000\n",
            "Generated 75500/100000\n",
            "Generated 75600/100000\n",
            "Generated 75700/100000\n",
            "Generated 75800/100000\n",
            "Generated 75900/100000\n",
            "Generated 76000/100000\n",
            "Generated 76100/100000\n",
            "Generated 76200/100000\n",
            "Generated 76300/100000\n",
            "Generated 76400/100000\n",
            "Generated 76500/100000\n",
            "Generated 76600/100000\n",
            "Generated 76700/100000\n",
            "Generated 76800/100000\n",
            "Generated 76900/100000\n",
            "Generated 77000/100000\n",
            "Generated 77100/100000\n",
            "Generated 77200/100000\n",
            "Generated 77300/100000\n",
            "Generated 77400/100000\n",
            "Generated 77500/100000\n",
            "Generated 77600/100000\n",
            "Generated 77700/100000\n",
            "Generated 77800/100000\n",
            "Generated 77900/100000\n",
            "Generated 78000/100000\n",
            "Generated 78100/100000\n",
            "Generated 78200/100000\n",
            "Generated 78300/100000\n",
            "Generated 78400/100000\n",
            "Generated 78500/100000\n",
            "Generated 78600/100000\n",
            "Generated 78700/100000\n",
            "Generated 78800/100000\n",
            "Generated 78900/100000\n",
            "Generated 79000/100000\n",
            "Generated 79100/100000\n",
            "Generated 79200/100000\n",
            "Generated 79300/100000\n",
            "Generated 79400/100000\n",
            "Generated 79500/100000\n",
            "Generated 79600/100000\n",
            "Generated 79700/100000\n",
            "Generated 79800/100000\n",
            "Generated 79900/100000\n",
            "Generated 80000/100000\n",
            "Generated 80100/100000\n",
            "Generated 80200/100000\n",
            "Generated 80300/100000\n",
            "Generated 80400/100000\n",
            "Generated 80500/100000\n",
            "Generated 80600/100000\n",
            "Generated 80700/100000\n",
            "Generated 80800/100000\n",
            "Generated 80900/100000\n",
            "Generated 81000/100000\n",
            "Generated 81100/100000\n",
            "Generated 81200/100000\n",
            "Generated 81300/100000\n",
            "Generated 81400/100000\n",
            "Generated 81500/100000\n",
            "Generated 81600/100000\n",
            "Generated 81700/100000\n",
            "Generated 81800/100000\n",
            "Generated 81900/100000\n",
            "Generated 82000/100000\n",
            "Generated 82100/100000\n",
            "Generated 82200/100000\n",
            "Generated 82300/100000\n",
            "Generated 82400/100000\n",
            "Generated 82500/100000\n",
            "Generated 82600/100000\n",
            "Generated 82700/100000\n",
            "Generated 82800/100000\n",
            "Generated 82900/100000\n",
            "Generated 83000/100000\n",
            "Generated 83100/100000\n",
            "Generated 83200/100000\n",
            "Generated 83300/100000\n",
            "Generated 83400/100000\n",
            "Generated 83500/100000\n",
            "Generated 83600/100000\n",
            "Generated 83700/100000\n",
            "Generated 83800/100000\n",
            "Generated 83900/100000\n",
            "Generated 84000/100000\n",
            "Generated 84100/100000\n",
            "Generated 84200/100000\n",
            "Generated 84300/100000\n",
            "Generated 84400/100000\n",
            "Generated 84500/100000\n",
            "Generated 84600/100000\n",
            "Generated 84700/100000\n",
            "Generated 84800/100000\n",
            "Generated 84900/100000\n",
            "Generated 85000/100000\n",
            "Generated 85100/100000\n",
            "Generated 85200/100000\n",
            "Generated 85300/100000\n",
            "Generated 85400/100000\n",
            "Generated 85500/100000\n",
            "Generated 85600/100000\n",
            "Generated 85700/100000\n",
            "Generated 85800/100000\n",
            "Generated 85900/100000\n",
            "Generated 86000/100000\n",
            "Generated 86100/100000\n",
            "Generated 86200/100000\n",
            "Generated 86300/100000\n",
            "Generated 86400/100000\n",
            "Generated 86500/100000\n",
            "Generated 86600/100000\n",
            "Generated 86700/100000\n",
            "Generated 86800/100000\n",
            "Generated 86900/100000\n",
            "Generated 87000/100000\n",
            "Generated 87100/100000\n",
            "Generated 87200/100000\n",
            "Generated 87300/100000\n",
            "Generated 87400/100000\n",
            "Generated 87500/100000\n",
            "Generated 87600/100000\n",
            "Generated 87700/100000\n",
            "Generated 87800/100000\n",
            "Generated 87900/100000\n",
            "Generated 88000/100000\n",
            "Generated 88100/100000\n",
            "Generated 88200/100000\n",
            "Generated 88300/100000\n",
            "Generated 88400/100000\n",
            "Generated 88500/100000\n",
            "Generated 88600/100000\n",
            "Generated 88700/100000\n",
            "Generated 88800/100000\n",
            "Generated 88900/100000\n",
            "Generated 89000/100000\n",
            "Generated 89100/100000\n",
            "Generated 89200/100000\n",
            "Generated 89300/100000\n",
            "Generated 89400/100000\n",
            "Generated 89500/100000\n",
            "Generated 89600/100000\n",
            "Generated 89700/100000\n",
            "Generated 89800/100000\n",
            "Generated 89900/100000\n",
            "Generated 90000/100000\n",
            "Generated 90100/100000\n",
            "Generated 90200/100000\n",
            "Generated 90300/100000\n",
            "Generated 90400/100000\n",
            "Generated 90500/100000\n",
            "Generated 90600/100000\n",
            "Generated 90700/100000\n",
            "Generated 90800/100000\n",
            "Generated 90900/100000\n",
            "Generated 91000/100000\n",
            "Generated 91100/100000\n",
            "Generated 91200/100000\n",
            "Generated 91300/100000\n",
            "Generated 91400/100000\n",
            "Generated 91500/100000\n",
            "Generated 91600/100000\n",
            "Generated 91700/100000\n",
            "Generated 91800/100000\n",
            "Generated 91900/100000\n",
            "Generated 92000/100000\n",
            "Generated 92100/100000\n",
            "Generated 92200/100000\n",
            "Generated 92300/100000\n",
            "Generated 92400/100000\n",
            "Generated 92500/100000\n",
            "Generated 92600/100000\n",
            "Generated 92700/100000\n",
            "Generated 92800/100000\n",
            "Generated 92900/100000\n",
            "Generated 93000/100000\n",
            "Generated 93100/100000\n",
            "Generated 93200/100000\n",
            "Generated 93300/100000\n",
            "Generated 93400/100000\n",
            "Generated 93500/100000\n",
            "Generated 93600/100000\n",
            "Generated 93700/100000\n",
            "Generated 93800/100000\n",
            "Generated 93900/100000\n",
            "Generated 94000/100000\n",
            "Generated 94100/100000\n",
            "Generated 94200/100000\n",
            "Generated 94300/100000\n",
            "Generated 94400/100000\n",
            "Generated 94500/100000\n",
            "Generated 94600/100000\n",
            "Generated 94700/100000\n",
            "Generated 94800/100000\n",
            "Generated 94900/100000\n",
            "Generated 95000/100000\n",
            "Generated 95100/100000\n",
            "Generated 95200/100000\n",
            "Generated 95300/100000\n",
            "Generated 95400/100000\n",
            "Generated 95500/100000\n",
            "Generated 95600/100000\n",
            "Generated 95700/100000\n",
            "Generated 95800/100000\n",
            "Generated 95900/100000\n",
            "Generated 96000/100000\n",
            "Generated 96100/100000\n",
            "Generated 96200/100000\n",
            "Generated 96300/100000\n",
            "Generated 96400/100000\n",
            "Generated 96500/100000\n",
            "Generated 96600/100000\n",
            "Generated 96700/100000\n",
            "Generated 96800/100000\n",
            "Generated 96900/100000\n",
            "Generated 97000/100000\n",
            "Generated 97100/100000\n",
            "Generated 97200/100000\n",
            "Generated 97300/100000\n",
            "Generated 97400/100000\n",
            "Generated 97500/100000\n",
            "Generated 97600/100000\n",
            "Generated 97700/100000\n",
            "Generated 97800/100000\n",
            "Generated 97900/100000\n",
            "Generated 98000/100000\n",
            "Generated 98100/100000\n",
            "Generated 98200/100000\n",
            "Generated 98300/100000\n",
            "Generated 98400/100000\n",
            "Generated 98500/100000\n",
            "Generated 98600/100000\n",
            "Generated 98700/100000\n",
            "Generated 98800/100000\n",
            "Generated 98900/100000\n",
            "Generated 99000/100000\n",
            "Generated 99100/100000\n",
            "Generated 99200/100000\n",
            "Generated 99300/100000\n",
            "Generated 99400/100000\n",
            "Generated 99500/100000\n",
            "Generated 99600/100000\n",
            "Generated 99700/100000\n",
            "Generated 99800/100000\n",
            "Generated 99900/100000\n",
            "Generated 100000/100000\n",
            "Writing to /content/drive/MyDrive/ailab/dpo/pos_neg_pairsSelfGenerate.json ...\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "OUTPUT_FILE = f\"{ROOT}/dpo/pos_neg_pairsSelfGenerate.json\"\n",
        "MIN_NUM, MAX_NUM = -127, 128\n",
        "NUM_SAMPLES = 100_000\n",
        "PRINT_EVERY = 100\n",
        "MAX_NEW_TOKENS = 50\n",
        "TEMPERATURE = 0.8\n",
        "TOP_K = 50\n",
        "\n",
        "ckpt_path = f\"{ROOT}/dpo/dpoTryNewE100k.pt\"\n",
        "checkpoint = torch.load(\n",
        "    ckpt_path,\n",
        "    map_location=lambda storage, loc: storage.cuda() if torch.cuda.is_available() else storage\n",
        ")\n",
        "gptconf = GPTConfig(**checkpoint[\"model_args\"])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gpt = GPT(gptconf).to(device)\n",
        "state_dict = checkpoint.get(\"model\", checkpoint.get(\"model_state_dict\", {}))\n",
        "unwanted_prefix = \"_orig_mod.\"\n",
        "for k, v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "gpt.eval()\n",
        "\n",
        "## Define all the math functions\n",
        "\n",
        "def rnd():\n",
        "    return random.randint(MIN_NUM, MAX_NUM)\n",
        "\n",
        "def small_nonzero(limit=500):\n",
        "    v = 0\n",
        "    while v == 0:\n",
        "        v = random.randint(-limit, limit)\n",
        "    return v\n",
        "\n",
        "def make_add():\n",
        "    a, b = rnd(), rnd()\n",
        "    q = f\"{a}+{b}=?\"\n",
        "    ans = a + b\n",
        "    pos = f\"{q} The answer is {ans} because {a}+{b} equals {ans}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_sub():\n",
        "    a, b = rnd(), rnd()\n",
        "    q = f\"{a}-{b}=?\"\n",
        "    ans = a - b\n",
        "    pos = f\"{q} The answer is {ans} because {a}-{b} equals {ans}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_mul():\n",
        "    a, b = rnd(), rnd()\n",
        "    q = f\"{a}*{b}=?\"\n",
        "    ans = a * b\n",
        "    pos = f\"{q} The answer is {ans} because {a}*{b} equals {ans}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_div():\n",
        "    b = small_nonzero()\n",
        "    q_int = rnd()\n",
        "    a = b * q_int\n",
        "    q = f\"{a}/{b}=?\"\n",
        "    pos = f\"{q} The answer is {q_int} because {a}/{b} equals {q_int}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_x_plus_a():\n",
        "    a, x = rnd(), rnd()\n",
        "    b = x + a\n",
        "    q = f\"x+{a}={b},x=?\"\n",
        "    pos = f\"{q} The answer is {x} because {b}-{a} equals {x}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_a_plus_x():\n",
        "    a, x = rnd(), rnd()\n",
        "    b = a + x\n",
        "    q = f\"{a}+x={b},x=?\"\n",
        "    pos = f\"{q} The answer is {x} because {b}-{a} equals {x}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_x_minus_a():\n",
        "    a, x = rnd(), rnd()\n",
        "    b = x - a\n",
        "    q = f\"x-{a}={b},x=?\"\n",
        "    pos = f\"{q} The answer is {x} because {b}+{a} equals {x}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_a_minus_x():\n",
        "    a, x = rnd(), rnd()\n",
        "    b = a - x\n",
        "    q = f\"{a}-x={b},x=?\"\n",
        "    pos = f\"{q} The answer is {x} because {a}-{b} equals {x}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_x_mul_a():\n",
        "    a = small_nonzero()\n",
        "    x = rnd()\n",
        "    b = x * a\n",
        "    q = f\"x*{a}={b},x=?\"\n",
        "    pos = f\"{q} The answer is {x} because {b}/{a} equals {x}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_a_mul_x():\n",
        "    a = small_nonzero()\n",
        "    x = rnd()\n",
        "    b = a * x\n",
        "    q = f\"{a}*x={b},x=?\"\n",
        "    pos = f\"{q} The answer is {x} because {b}/{a} equals {x}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_x_div_a():\n",
        "    a = small_nonzero()\n",
        "    b = rnd()\n",
        "    x = a * b\n",
        "    q = f\"x/{a}={b},x=?\"\n",
        "    pos = f\"{q} The answer is {x} because {b}*{a} equals {x}.\"\n",
        "    return q, pos\n",
        "\n",
        "def make_a_div_x():\n",
        "    x = small_nonzero()\n",
        "    b = rnd()\n",
        "    a = b * x\n",
        "    q = f\"{a}/x={b},x=?\"\n",
        "    pos = f\"{q} The answer is {x} because {a}/{b} equals {x}.\"\n",
        "    return q, pos\n",
        "\n",
        "GEN_FUNCS = [\n",
        "    make_add, make_sub, make_mul, make_div,\n",
        "    make_x_plus_a, make_a_plus_x, make_x_minus_a, make_a_minus_x,\n",
        "    make_x_mul_a, make_a_mul_x, make_x_div_a, make_a_div_x\n",
        "]\n",
        "\n",
        "##get the model to generate the negative\n",
        "\n",
        "def model_generate(prompt):\n",
        "    prompt_ids = encode(prompt)\n",
        "    x = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        y_tuple = gpt.generate(x, max_new_tokens=MAX_NEW_TOKENS,\n",
        "                               temperature=TEMPERATURE, top_k=TOP_K)\n",
        "    y = y_tuple[0][0].tolist()\n",
        "    generated_ids = y[len(prompt_ids):]\n",
        "    return decode(generated_ids).strip()\n",
        "\n",
        "##combine them together\n",
        "\n",
        "def generate_dataset(n):\n",
        "    data = []\n",
        "    for i in range(n):\n",
        "        func = random.choice(GEN_FUNCS)\n",
        "        q, pos = func()\n",
        "        neg = f\"{q} {model_generate(q)}\"\n",
        "        data.append({\"negative\": neg, \"positive\": pos})\n",
        "        if (i + 1) % PRINT_EVERY == 0:\n",
        "            print(f\"Generated {i + 1}/{n}\")\n",
        "    return data\n",
        "\n",
        "##write in the json format required\n",
        "\n",
        "def write_strict_json(filepath, entries):\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"[\")\n",
        "        for idx, e in enumerate(entries):\n",
        "            neg_json = json.dumps(e[\"negative\"], ensure_ascii=False)\n",
        "            pos_json = json.dumps(e[\"positive\"], ensure_ascii=False)\n",
        "            obj_line = \"{\" + f\"\\\"negative\\\": {neg_json},\\t\\\"positive\\\": {pos_json}\" + \"}\"\n",
        "            f.write(obj_line)\n",
        "            if idx != len(entries) - 1:\n",
        "                f.write(\",\\n\")\n",
        "        f.write(\"]\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Generating {NUM_SAMPLES} self-evaluated samples ...\")\n",
        "    dataset = generate_dataset(NUM_SAMPLES)\n",
        "    print(f\"Writing to {OUTPUT_FILE} ...\")\n",
        "    write_strict_json(OUTPUT_FILE, dataset)\n",
        "    print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8653d7",
      "metadata": {},
      "source": [
        "# Generation code of more similar positive-negative pair\n",
        "Next, since the negative part of the previous set performs so bad that the delta is extremely large, we tried to generate more similar positive negative pairs like the following {\"negative\": \"x/143=55,x=? The answer is 7850 because 55*143 equals 7850.\",\n",
        "    \"positive\": \"x/143=55,x=? The answer is 7865 because 55*143 equals 7865.\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "73fe338ab2a2dcb5",
      "metadata": {
        "id": "73fe338ab2a2dcb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 100,000 math problem pairs with distinct positive and negative explanations and saved to pos_neg_similar.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "def generate_math_problem():\n",
        "    operations = ['+', '-', '*', '/']\n",
        "    op = random.choice(operations)\n",
        "    a = random.randint(-1000, 1000)\n",
        "    b = random.randint(-1000, 1000)\n",
        "    \n",
        "    # Ensure no division by zero\n",
        "    if op == '/':\n",
        "        while b == 0:\n",
        "            b = random.randint(-1000, 1000)\n",
        "    \n",
        "    # Calculate correct answer and explanation\n",
        "    if op == '+':\n",
        "        correct_answer = a + b\n",
        "        problem = f\"{a}+{b}=?\"\n",
        "        correct_explanation = f\"{a}+{b} equals {correct_answer}\"\n",
        "        # Negative: Common mistake (e.g., subtracting instead of adding)\n",
        "        incorrect_answer = a - b\n",
        "        incorrect_explanation = f\"{a}-{b} equals {incorrect_answer}\"\n",
        "    elif op == '-':\n",
        "        correct_answer = a - b\n",
        "        problem = f\"{a}-{b}=?\"\n",
        "        correct_explanation = f\"{a}-{b} equals {correct_answer}\"\n",
        "        # Negative: Common mistake (e.g., adding instead of subtracting)\n",
        "        incorrect_answer = a + b\n",
        "        incorrect_explanation = f\"{a}+{b} equals {incorrect_answer}\"\n",
        "    elif op == '*':\n",
        "        correct_answer = a * b\n",
        "        problem = f\"{a}*{b}=?\"\n",
        "        correct_explanation = f\"{a}*{b} equals {correct_answer}\"\n",
        "        # Negative: Common mistake (e.g., adding instead of multiplying)\n",
        "        incorrect_answer = a + b\n",
        "        incorrect_explanation = f\"{a}+{b} equals {incorrect_answer}\"\n",
        "    else:  # Division\n",
        "        # Ensure clean division (no decimals)\n",
        "        correct_answer = a // b\n",
        "        a = correct_answer * b  # Adjust a to ensure integer division\n",
        "        problem = f\"{a}/{b}=?\"\n",
        "        correct_explanation = f\"{a}/{b} equals {correct_answer}\"\n",
        "        # Negative: Common mistake (e.g., multiplying instead of dividing)\n",
        "        incorrect_answer = a * b\n",
        "        incorrect_explanation = f\"{a}*{b} equals {incorrect_answer}\"\n",
        "    \n",
        "    # Ensure incorrect answer is different\n",
        "    if incorrect_answer == correct_answer:\n",
        "        incorrect_answer = correct_answer + random.choice([-1, 1])\n",
        "        incorrect_explanation = f\"{a}{op}{b} miscalculated as {incorrect_answer}\"\n",
        "    \n",
        "    positive = f\"{problem} The answer is {correct_answer} because {correct_explanation}.\"\n",
        "    negative = f\"{problem} The answer is {incorrect_answer} because {incorrect_explanation}.\"\n",
        "    \n",
        "    return {\n",
        "        \"positive\": positive,\n",
        "        \"negative\": negative\n",
        "    }\n",
        "\n",
        "# Generate 100,000 problem pairs\n",
        "data = [generate_math_problem() for _ in range(100000)]\n",
        "\n",
        "# Save to JSON file\n",
        "with open('pos_neg_similar.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Generated 100,000 math problem pairs with distinct positive and negative explanations and saved to pos_neg_similar.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91a13577",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
